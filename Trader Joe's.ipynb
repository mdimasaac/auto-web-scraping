{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc21edbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdima\\AppData\\Local\\Temp\\ipykernel_872\\2380066540.py:25: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = 'C:\\\\Users\\\\mdima\\\\Desktop\\\\Python Learning Files\\\\02\\\\chromedriver.exe', options = option1)\n"
     ]
    }
   ],
   "source": [
    "# --installing a few stuffs first--\n",
    "# pip install pandas\n",
    "# pip install requests\n",
    "# pip install bs4\n",
    "# pip install geopy\n",
    "# pip install selenium\n",
    "# for installing selenium webdriver, see these 2 youtube tutorials for references:\n",
    "# https://bit.ly/3D0InrZ\n",
    "# https://bit.ly/3wnvGWS\n",
    "# --importing pandas--\n",
    "import os\n",
    "import pandas\n",
    "# --importing web scraping stuff--\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# --importing geopy stuff--\n",
    "from geopy.geocoders import ArcGIS\n",
    "# --importing selenium webdriver for opening and navigating through webpages--\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# --importing time for making action delays (if needed)--\n",
    "import time\n",
    "\n",
    "# --stuff for geopy--\n",
    "nom = ArcGIS()\n",
    "# --creating a new empty dataframe with just columns\n",
    "df = pandas.DataFrame(columns=['Street Address','City','State','ZIP Code','Location','Coordinates','Latitude','Longitude'])\n",
    "# --disabling browser popup notifications--\n",
    "option1 = Options()\n",
    "option1.add_argument(\"--disable-notifications\")\n",
    "# --locating chromedriver in directory--\n",
    "driver = webdriver.Chrome(executable_path = 'C:\\\\Users\\\\mdima\\\\Desktop\\\\Python Learning Files\\\\02\\\\chromedriver.exe', options = option1)\n",
    "# --opening browser and the homepage--\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://locations.traderjoes.com\")\n",
    "# --defining index for later iteration--\n",
    "index200 = list(range(0,200))\n",
    "# --empty list to save multiple xpath from the page--\n",
    "xpath_list = []\n",
    "# --making a list of clickable xpaths--\n",
    "for item in index200:\n",
    "    xpath = '/html/body/div[2]/div[2]/main/div/div/div/div[2]/div[4]/div/div/div/div[2]/div/div[%s]/a'%str(index200.index(item)+1)\n",
    "    xpath_list.append(xpath)\n",
    "# --empty list to save all the stores' datas (address, zip code etc.)--\n",
    "l = []\n",
    "# --empty list to store all urls of the cities--\n",
    "stores = []\n",
    "for item in xpath_list:\n",
    "    try:\n",
    "        # --finding and clicking every states in the homepage--\n",
    "        # --moving to city page after clicking--\n",
    "        # --will stop finding and clicking once theres no more clickable states--\n",
    "        item2 = xpath_list[0]\n",
    "        link = driver.find_element(By.XPATH, item)\n",
    "        link.click()\n",
    "        for item2 in xpath_list:     \n",
    "            try:\n",
    "                # --finding and clicking every cities in the state page--\n",
    "                # --moving to stores page after clicking--\n",
    "                # --will stop finding and clicking once theres no more clickable cities--\n",
    "                link2 = driver.find_element(By.XPATH, item2)\n",
    "                link2.click()\n",
    "                # --getting the url of the clicked city, which has all infos for the stores--\n",
    "                current = driver.current_url\n",
    "                # --the urls will then be saved in the list for further use--\n",
    "                stores.append(current)\n",
    "                # --not much more to do here, so we go back one page from store page to city page--\n",
    "                driver.back()\n",
    "            except:\n",
    "                # --when theres no more clickable city, then go back one page from city page to homepage--\n",
    "                driver.back()\n",
    "                break\n",
    "    except:\n",
    "        break\n",
    "# --now we got the urls from each city. each url will be soup'ed--\n",
    "for store in stores:\n",
    "    # --beautifulsoup stuff--\n",
    "    r = requests.get(store)\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup (c, \"html.parser\")\n",
    "    # --in the store page, inspect the block with the store infos in it--\n",
    "    # --it has a \"div\" tag with class \"address-left\"--\n",
    "    all = soup.find_all(\"div\",{\"class\":\"address-left\"})\n",
    "    for i in all:\n",
    "        d = {}\n",
    "        # --web scraping stuff with BeautifulSoup--\n",
    "        Address = i.find_all(\"span\")[1].text\n",
    "        City = i.find_all(\"span\")[2].text\n",
    "        State_l = soup.find(\"div\",{\"class\":\"breadcrumb container\"}).find_all(\"a\")[2].text\n",
    "        State_s = i.find_all(\"span\")[3].text\n",
    "        Zipcode = i.find_all(\"span\")[4].text\n",
    "        Country = i.find_all(\"span\")[5].text\n",
    "        # --to obtain latitude and longitude, we need this arrangement for the data--\n",
    "        Location = Address+\", \"+City+\", \"+State_s+\" \"+Zipcode+\", \"+Country\n",
    "        # --d is a dictionary. so we add values to them keys--\n",
    "        # --d[\"key\"] = value--\n",
    "        d[\"Street Address\"] = Address\n",
    "        d[\"City\"] = City\n",
    "        d[\"State\"] = State_l\n",
    "        d[\"ZIP Code\"] = Zipcode\n",
    "        d[\"Location\"] = Location\n",
    "        # --latitude and longitude function can only be used once the data has been transformed in dataframe form--\n",
    "        # --adding the elements of the dictionary to the previously prepared empty list--\n",
    "        # --because we need a list to make a dataframe--\n",
    "        l.append(d)\n",
    "# --now transferring all the data from the list l to the empty dataframe df we made earlier--       \n",
    "df = pandas.DataFrame(l)\n",
    "# --geopy functions work only with dataframes (probably)--\n",
    "# --turning the location into coordinates\n",
    "df[\"Coordinates\"] = df[\"Location\"].apply(nom.geocode)\n",
    "# --extracting latitude values from the coordinates--\n",
    "df[\"Latitude\"] = df[\"Coordinates\"].apply(lambda x: x.latitude if x != None else None)\n",
    "# --extracting longitude values from the coordinates--\n",
    "df[\"Longitude\"] = df[\"Coordinates\"].apply(lambda x: x.longitude if x != None else None)\n",
    "# --try printing df to see your work and be proud--\n",
    "# --saving the finished dataframe to csv\n",
    "df.to_csv(\"Trader Joes store data.csv\")\n",
    "# --saving the finished dataframe to excel\n",
    "df.to_excel(\"Trader Joes store data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec680a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
